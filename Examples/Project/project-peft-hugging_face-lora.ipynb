{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad37bc09",
   "metadata": {},
   "source": [
    "# PEFT Fine-Tuning with DistilBERT (LoRA)\n",
    "\n",
    "This notebook follows the project instructions: load a foundation model, evaluate it, fine-tune with PEFT (LoRA), and compare inference results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8732be",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ade0c841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import logging\n",
    "# Suppress informational messages about newly-initialized heads; training will still be required\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b67f6909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c775a869",
   "metadata": {},
   "source": [
    "## Load Dataset (Cleanlab/amazon-reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d886de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review_text', 'label'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review_text', 'label'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a small subset for quick iteration\n",
    "raw_ds = load_dataset(\"cleanlab/amazon-reviews\")\n",
    "\n",
    "# Inspect available splits and columns\n",
    "raw_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a82e8011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['review_text', 'label']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the train split and take a small subset\n",
    "train_ds = raw_ds[\"train\"].shuffle(seed=seed).select(range(2000))\n",
    "eval_ds = raw_ds[\"train\"].shuffle(seed=seed + 1).select(range(500))\n",
    "\n",
    "# Identify columns (common fields include 'text' and 'label')\n",
    "train_ds.column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca7ce15",
   "metadata": {},
   "source": [
    "## Tokenization and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "408ce84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Ensure pad token exists\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "text_column = \"text\" if \"text\" in train_ds.column_names else train_ds.column_names[0]\n",
    "label_column = \"label\" if \"label\" in train_ds.column_names else train_ds.column_names[-1]\n",
    "\n",
    "# Build deterministic label2id mapping from the raw train split (preserve order)\n",
    "raw_labels = list(dict.fromkeys(train_ds[label_column]))\n",
    "label2id = {lab: i for i, lab in enumerate(raw_labels)}\n",
    "num_labels = len(label2id)\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch[text_column], truncation=True, padding=False)\n",
    "\n",
    "tokenized_train = train_ds.map(tokenize_fn, batched=True, remove_columns=[text_column])\n",
    "tokenized_eval = eval_ds.map(tokenize_fn, batched=True, remove_columns=[text_column])\n",
    "\n",
    "tokenized_train = tokenized_train.rename_column(label_column, \"labels\")\n",
    "tokenized_eval = tokenized_eval.rename_column(label_column, \"labels\")\n",
    "\n",
    "def _convert_labels_to_int(batch):\n",
    "    new_labels = []\n",
    "    for x in batch['labels']:\n",
    "        if isinstance(x, str):\n",
    "            new_labels.append(label2id[x])\n",
    "        else:\n",
    "            new_labels.append(int(x))\n",
    "    batch['labels'] = new_labels\n",
    "    return batch\n",
    "\n",
    "tokenized_train = tokenized_train.map(_convert_labels_to_int, batched=True)\n",
    "tokenized_eval = tokenized_eval.map(_convert_labels_to_int, batched=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d465dc",
   "metadata": {},
   "source": [
    "## Load and Evaluate Foundation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da26abe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb1aa5c6e2c3431488f0747d6b2b605a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': '0.6988', 'eval_model_preparation_time': '0', 'eval_accuracy': '0.39', 'eval_runtime': '0.6151', 'eval_samples_per_second': '812.8', 'eval_steps_per_second': '102.4'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6987676024436951,\n",
       " 'eval_model_preparation_time': 0.0,\n",
       " 'eval_accuracy': 0.39,\n",
       " 'eval_runtime': 0.6151,\n",
       " 'eval_samples_per_second': 812.839,\n",
       " 'eval_steps_per_second': 102.418}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine number of labels from dataset (use label2id if available)\n",
    "num_labels = len(label2id) if 'label2id' in globals() else len(set(tokenized_train[\"labels\"]))\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "base_model.to(device)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    accuracy = (preds == labels).mean().item()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "base_args = TrainingArguments(\n",
    "    output_dir=\"./outputs_base\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    do_train=False,\n",
    "    do_eval=True,\n",
    "    logging_strategy=\"no\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "base_trainer = Trainer(\n",
    "    model=base_model,\n",
    "    args=base_args,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "base_metrics = base_trainer.evaluate()\n",
    "base_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a220b3d0",
   "metadata": {},
   "source": [
    "## Create PEFT (LoRA) Config and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "717119aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found candidate module names (showing up to 40):\n",
      "['distilbert.transformer.layer.0.attention.q_lin', 'distilbert.transformer.layer.0.attention.k_lin', 'distilbert.transformer.layer.0.attention.v_lin', 'distilbert.transformer.layer.1.attention.q_lin', 'distilbert.transformer.layer.1.attention.k_lin', 'distilbert.transformer.layer.1.attention.v_lin', 'distilbert.transformer.layer.2.attention.q_lin', 'distilbert.transformer.layer.2.attention.k_lin', 'distilbert.transformer.layer.2.attention.v_lin', 'distilbert.transformer.layer.3.attention.q_lin', 'distilbert.transformer.layer.3.attention.k_lin', 'distilbert.transformer.layer.3.attention.v_lin', 'distilbert.transformer.layer.4.attention.q_lin', 'distilbert.transformer.layer.4.attention.k_lin', 'distilbert.transformer.layer.4.attention.v_lin', 'distilbert.transformer.layer.5.attention.q_lin', 'distilbert.transformer.layer.5.attention.k_lin', 'distilbert.transformer.layer.5.attention.v_lin']\n",
      "Using LoRA target modules: ['q_lin', 'k_lin', 'v_lin']\n",
      "trainable params: 813,314 || all params: 67,768,324 || trainable%: 1.2001\n"
     ]
    }
   ],
   "source": [
    "# Quick diagnostic: list module names that look like attention projections (query/key/value/q_lin/v_lin)\n",
    "candidates = []\n",
    "for name, module in base_model.named_modules():\n",
    "    lname = name.lower()\n",
    "    if any(k in lname for k in (\"query\", \"key\", \"value\", \"q_lin\", \"k_lin\", \"v_lin\", \"attn\")):\n",
    "        candidates.append(name)\n",
    "\n",
    "print('Found candidate module names (showing up to 40):')\n",
    "print(candidates[:40])\n",
    "\n",
    "# Preferred target modules for encoder models (DistilBERT/BERT typically use 'query','key','value')\n",
    "preferred_targets = [\"query\", \"key\", \"value\"]\n",
    "# Fallback targets if model uses different attribute names\n",
    "fallback_targets = [\"q_lin\", \"k_lin\", \"v_lin\"]\n",
    "# Choose targets that appear in candidate module names; otherwise use preferred targets\n",
    "selected = []\n",
    "for t in preferred_targets + fallback_targets:\n",
    "    if any(t in cname.lower() for cname in candidates):\n",
    "        selected.append(t)\n",
    "if not selected:\n",
    "    selected = preferred_targets\n",
    "\n",
    "print('Using LoRA target modules:', selected)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=selected,\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(base_model, lora_config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d6cbb1",
   "metadata": {},
   "source": [
    "## Train the PEFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9d77cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': '0.6716', 'grad_norm': '3.329', 'learning_rate': '1.869e-05', 'epoch': '0.2'}\n",
      "{'loss': '0.6237', 'grad_norm': '1.677', 'learning_rate': '1.736e-05', 'epoch': '0.4'}\n",
      "{'loss': '0.6156', 'grad_norm': '1.662', 'learning_rate': '1.603e-05', 'epoch': '0.6'}\n",
      "{'loss': '0.5663', 'grad_norm': '1.37', 'learning_rate': '1.469e-05', 'epoch': '0.8'}\n",
      "{'loss': '0.5342', 'grad_norm': '1.626', 'learning_rate': '1.336e-05', 'epoch': '1'}\n",
      "{'eval_loss': '0.4814', 'eval_accuracy': '0.756', 'eval_runtime': '0.4556', 'eval_samples_per_second': '1097', 'eval_steps_per_second': '138.3', 'epoch': '1'}\n",
      "{'loss': '0.4625', 'grad_norm': '1.084', 'learning_rate': '1.203e-05', 'epoch': '1.2'}\n",
      "{'loss': '0.4227', 'grad_norm': '1.405', 'learning_rate': '1.069e-05', 'epoch': '1.4'}\n",
      "{'loss': '0.4276', 'grad_norm': '1.361', 'learning_rate': '9.36e-06', 'epoch': '1.6'}\n",
      "{'loss': '0.3912', 'grad_norm': '1.895', 'learning_rate': '8.027e-06', 'epoch': '1.8'}\n",
      "{'loss': '0.3812', 'grad_norm': '2.807', 'learning_rate': '6.693e-06', 'epoch': '2'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': '0.3928', 'eval_accuracy': '0.814', 'eval_runtime': '0.4633', 'eval_samples_per_second': '1079', 'eval_steps_per_second': '136', 'epoch': '2'}\n",
      "{'loss': '0.3612', 'grad_norm': '2.594', 'learning_rate': '5.36e-06', 'epoch': '2.2'}\n",
      "{'loss': '0.3856', 'grad_norm': '2.588', 'learning_rate': '4.027e-06', 'epoch': '2.4'}\n",
      "{'loss': '0.4307', 'grad_norm': '3.545', 'learning_rate': '2.693e-06', 'epoch': '2.6'}\n",
      "{'loss': '0.3558', 'grad_norm': '1.901', 'learning_rate': '1.36e-06', 'epoch': '2.8'}\n",
      "{'loss': '0.3467', 'grad_norm': '2.989', 'learning_rate': '2.667e-08', 'epoch': '3'}\n",
      "{'eval_loss': '0.3923', 'eval_accuracy': '0.818', 'eval_runtime': '0.4358', 'eval_samples_per_second': '1147', 'eval_steps_per_second': '144.6', 'epoch': '3'}\n",
      "{'train_runtime': '15.11', 'train_samples_per_second': '397.1', 'train_steps_per_second': '49.64', 'train_loss': '0.4651', 'epoch': '3'}\n",
      "Saved PEFT adapters to ./distilbert-lora-finetuned\n",
      "{'eval_loss': '0.3923', 'eval_accuracy': '0.818', 'eval_runtime': '0.4242', 'eval_samples_per_second': '1179', 'eval_steps_per_second': '148.5', 'epoch': '3'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3923342227935791,\n",
       " 'eval_accuracy': 0.818,\n",
       " 'eval_runtime': 0.4242,\n",
       " 'eval_samples_per_second': 1178.795,\n",
       " 'eval_steps_per_second': 148.528,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./outputs_peft\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_accuracy\",\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Ensure label mappings exist in the model config (helps Trainer and checkpoints)\n",
    "if not hasattr(peft_model.config, \"id2label\") or not hasattr(peft_model.config, \"label2id\"):\n",
    "    peft_model.config.id2label = {i: str(i) for i in range(num_labels)}\n",
    "    peft_model.config.label2id = {str(i): i for i in range(num_labels)}\n",
    "\n",
    "# Move PEFT model to the appropriate device\n",
    "peft_model.to(device)\n",
    "\n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the PEFT model (this updates the randomly-initialized classification head)\n",
    "peft_trainer.train()\n",
    "\n",
    "# Save the PEFT adapters (and optionally the full model if desired)\n",
    "save_dir = \"./distilbert-lora-finetuned\"\n",
    "peft_model.save_pretrained(save_dir)\n",
    "print(\"Saved PEFT adapters to\", save_dir)\n",
    "\n",
    "# Evaluate after fine-tuning\n",
    "peft_metrics = peft_trainer.evaluate()\n",
    "peft_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fbbb9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': '0.3923', 'eval_accuracy': '0.818', 'eval_runtime': '0.4445', 'eval_samples_per_second': '1125', 'eval_steps_per_second': '141.7', 'epoch': '3'}\n",
      "PEFT Model Evaluation Metrics:\n",
      "Eval Loss: 0.3923342227935791\n",
      "Eval Accuracy: 0.8180\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the fine-tuned PEFT model\n",
    "peft_metrics = peft_trainer.evaluate()\n",
    "print(\"PEFT Model Evaluation Metrics:\")\n",
    "print(f\"Eval Loss: {peft_metrics.get('eval_loss', 'N/A')}\")\n",
    "print(f\"Eval Accuracy: {peft_metrics.get('eval_accuracy', 'N/A'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0dad6b",
   "metadata": {},
   "source": [
    "## Evaluate Fine-Tuned PEFT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1589ea0",
   "metadata": {},
   "source": [
    "## Save the Trained PEFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "637f65b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./distilbert-lora'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir = \"./distilbert-lora\"\n",
    "peft_model.save_pretrained(save_dir)\n",
    "save_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8687d5a",
   "metadata": {},
   "source": [
    "### Alternative Save Location\n",
    "\n",
    "Save a copy of the trained model to a separate directory for reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e856c80",
   "metadata": {},
   "source": [
    "## Evaluate the PEFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abdf8e82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77e81e300cfa435aaf93a7b7634ffc1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded fine-tuned PEFT model from ./distilbert-lora-finetuned\n",
      "Model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForSequenceClassification\n",
    "\n",
    "# Load the saved PEFT model from the fine-tuned directory\n",
    "peft_model_loaded = AutoPeftModelForSequenceClassification.from_pretrained(\n",
    "    \"./distilbert-lora-finetuned\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Move to device if needed\n",
    "peft_model_loaded.to(device)\n",
    "print(\"Loaded fine-tuned PEFT model from ./distilbert-lora-finetuned\")\n",
    "print(f\"Model device: {next(peft_model_loaded.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70fe548c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': '0.3923', 'eval_model_preparation_time': '0.001', 'eval_accuracy': '0.818', 'eval_runtime': '0.4486', 'eval_samples_per_second': '1115', 'eval_steps_per_second': '140.4'}\n",
      "Loaded PEFT model evaluation metrics: {'eval_loss': 0.3923342227935791, 'eval_model_preparation_time': 0.001, 'eval_accuracy': 0.818, 'eval_runtime': 0.4486, 'eval_samples_per_second': 1114.544, 'eval_steps_per_second': 140.432}\n"
     ]
    }
   ],
   "source": [
    "# Perform inference using the loaded PEFT model\n",
    "peft_trainer_loaded = Trainer(\n",
    "    model=peft_model_loaded,\n",
    "    args=base_args,  # Use same eval args as base model for fair comparison\n",
    "    eval_dataset=tokenized_eval,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Evaluate the loaded PEFT model\n",
    "peft_metrics_loaded = peft_trainer_loaded.evaluate()\n",
    "print(\"Loaded PEFT model evaluation metrics:\", peft_metrics_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d18aa3",
   "metadata": {},
   "source": [
    "## Load and Use the Saved PEFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "933e658d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': '0.3923', 'eval_accuracy': '0.818', 'eval_runtime': '0.4527', 'eval_samples_per_second': '1104', 'eval_steps_per_second': '139.2', 'epoch': '3'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3923342227935791,\n",
       " 'eval_accuracy': 0.818,\n",
       " 'eval_runtime': 0.4527,\n",
       " 'eval_samples_per_second': 1104.437,\n",
       " 'eval_steps_per_second': 139.159,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_metrics = peft_trainer.evaluate()\n",
    "peft_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec1c27e",
   "metadata": {},
   "source": [
    "### Re-evaluate Using Trainer\n",
    "\n",
    "Verify metrics by running evaluation through the trainer again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f13e72",
   "metadata": {},
   "source": [
    "## Summary: Compare Base vs. Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c0f9ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "COMPARISON: Base Model vs. Fine-Tuned PEFT Model\n",
      "============================================================\n",
      "            Model  Eval Loss  Accuracy\n",
      "Base (Pretrained)   0.698768     0.390\n",
      "  PEFT Fine-Tuned   0.392334     0.818\n",
      "============================================================\n",
      "\n",
      "Accuracy Improvement: +42.80%\n",
      "Loss Reduction: 0.3064\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a comparison table\n",
    "comparison_data = {\n",
    "    \"Model\": [\"Base (Pretrained)\", \"PEFT Fine-Tuned\"],\n",
    "    \"Eval Loss\": [base_metrics.get(\"eval_loss\", \"N/A\"), peft_metrics.get(\"eval_loss\", \"N/A\")],\n",
    "    \"Accuracy\": [base_metrics.get(\"eval_accuracy\", \"N/A\"), peft_metrics.get(\"eval_accuracy\", \"N/A\")],\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON: Base Model vs. Fine-Tuned PEFT Model\")\n",
    "print(\"=\"*60)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Calculate improvement\n",
    "if isinstance(base_metrics.get(\"eval_accuracy\"), (int, float)) and isinstance(peft_metrics.get(\"eval_accuracy\"), (int, float)):\n",
    "    accuracy_improvement = (peft_metrics[\"eval_accuracy\"] - base_metrics[\"eval_accuracy\"]) * 100\n",
    "    print(f\"Accuracy Improvement: {accuracy_improvement:+.2f}%\")\n",
    "    \n",
    "if isinstance(base_metrics.get(\"eval_loss\"), (int, float)) and isinstance(peft_metrics.get(\"eval_loss\"), (int, float)):\n",
    "    loss_improvement = base_metrics[\"eval_loss\"] - peft_metrics[\"eval_loss\"]\n",
    "    print(f\"Loss Reduction: {loss_improvement:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4335c2f0",
   "metadata": {},
   "source": [
    "QLoRA (Quantized LoRA) for memory-efficient fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6643aaa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336321e784744e51a2f04eb38f79e568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 813,314 || all params: 67,768,324 || trainable%: 1.2001\n",
      "\n",
      "✓ QLoRA model successfully created (4-bit quantized + LoRA)\n",
      "This configuration is more memory-efficient for large models!\n"
     ]
    }
   ],
   "source": [
    "# Optional: QLoRA (Quantized LoRA) for memory-efficient fine-tuning\n",
    "try:\n",
    "    from transformers import BitsAndBytesConfig\n",
    "    from peft import prepare_model_for_kbit_training\n",
    "    \n",
    "    # Configure 4-bit quantization\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "    )\n",
    "    \n",
    "    # Load base model with quantization\n",
    "    base_model_qlora = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_labels,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    \n",
    "    # Prepare model for k-bit training\n",
    "    base_model_qlora = prepare_model_for_kbit_training(base_model_qlora)\n",
    "    \n",
    "    # Apply LoRA on top of quantized model\n",
    "    lora_config_qlora = LoraConfig(\n",
    "        task_type=\"SEQ_CLS\",\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        target_modules=selected,  # Use same targets as before\n",
    "    )\n",
    "    \n",
    "    peft_model_qlora = get_peft_model(base_model_qlora, lora_config_qlora)\n",
    "    peft_model_qlora.print_trainable_parameters()\n",
    "    \n",
    "    print(\"\\n✓ QLoRA model successfully created (4-bit quantized + LoRA)\")\n",
    "    print(\"This configuration is more memory-efficient for large models!\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"⚠️ bitsandbytes not available for QLoRA demo: {e}\")\n",
    "    print(\"Install with: pip install bitsandbytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1934e7a2",
   "metadata": {},
   "source": [
    "### QLoRA Implementation\n",
    "\n",
    "Optional: Combine 4-bit quantization with LoRA for memory-efficient training on larger models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10c5faa",
   "metadata": {},
   "source": [
    "## Bonus: QLoRA Fine-Tuning Example\n",
    "\n",
    "This section demonstrates QLoRA (Quantized LoRA), which combines quantization with LoRA for memory-efficient training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
